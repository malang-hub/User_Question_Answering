# ğŸ“˜ Descriptive Overview: BERT Question Answering System

**A Colab-based deep learning project that fine-tunes BERT on the SQuAD dataset to answer questions from context paragraphs.**

This project demonstrates how to build a **BERT-based Question Answering (QA) model** from scratch using the **SQuAD v1.1 dataset**. The core objective is to enable machines to answer natural language questions based on a given paragraph or context. It leverages **transfer learning** with the `bert-base-uncased` model provided by Hugging Face.

The QA pipeline involves:

* Loading the dataset
* Preprocessing it to match BERT's format
* Fine-tuning BERT on the QA task
* Predicting answers from context

This notebook is designed to run on **Google Colab**, supports **GPU acceleration**, and stores results on **Google Drive**.

## ğŸ“‚ Project Structure

```
BERT_QA_System/
â”œâ”€â”€ squad_train.csv                # Exported training data (SQuAD)
â”œâ”€â”€ squad_validation.csv           # Exported validation data
â”œâ”€â”€ tokenized_squad_train/         # Tokenized training dataset (HF format)
â”œâ”€â”€ tokenized_squad_val/           # Tokenized validation dataset (HF format)
â”œâ”€â”€ bert-qa/                       # Output directory for the trained model
â”œâ”€â”€ logs/                          # Training logs
â”œâ”€â”€ QA_script.ipynb                # Main Colab notebook with code
â””â”€â”€ README.md                      # This file
```

## ğŸš€ Features

* âœ… Load and preprocess the SQuAD dataset
* âœ… Tokenize using `BertTokenizerFast`
* âœ… Fine-tune `BertForQuestionAnswering`
* âœ… Predict answers from context paragraphs
* âœ… Save and reload tokenized datasets
* âœ… Optional integration with Weights & Biases for tracking

## ğŸ›  Requirements

* Python 3.7+
* Transformers â‰¥ 4.28
* Datasets â‰¥ 2.10
* Torch â‰¥ 1.12
* Google Colab (preferred)

## ğŸ§ª Usage Instructions

1. Open the notebook in Google Colab.
2. Enable GPU: **Runtime > Change runtime type > GPU**.
3. Run each cell in order.
4. Mount Google Drive to save/export your model and datasets.
5. (Optional) Reduce dataset size for faster testing.
6. Train the model for 1 epoch.
7. Use `predict_answer()` to infer answers.

## ğŸ§  Model Description

* We use `bert-base-uncased`, a transformer model pretrained on a large corpus of English data.
* It is fine-tuned on the SQuAD dataset, which consists of questions and answer spans within context paragraphs.
* The model learns to predict the start and end token positions of the answer in the context.

## ğŸ”§ Customization

* âœï¸ Replace default question/context with your own for testing.
* ğŸ’¾ Save the model with `model.save_pretrained()`.
* ğŸ” Reload later with `from_pretrained()`.

## ğŸ“ˆ Optional: Tracking with Weights & Biases

Enable experiment tracking by:

```python
import wandb
wandb.login()  # Youâ€™ll be prompted for API key
```

To disable logging:

```python
import os
os.environ["WANDB_DISABLED"] = "true"
```

## ğŸ“š References

* [SQuAD Dataset](https://rajpurkar.github.io/SQuAD-explorer/)
* [Hugging Face Transformers](https://huggingface.co/transformers/)
* [Hugging Face Datasets](https://huggingface.co/docs/datasets/)
* [Google Colab](https://colab.research.google.com)

---

**Author:** Aditya Verma 
**Date:** July 2025
